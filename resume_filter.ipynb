{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    简历过滤器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用提供的数据集训练一个模型，用来过滤求职者的简历；\n",
    "用来判断求职者是：不能被录取、适合当开发工程师、适合当测试工程师，还是适合当经理\n",
    "验证标准：精准率，召回率和 F1Score。\n",
    "\n",
    "1.尝试使用 sklearn 的 逻辑回归模型，SVM模型，以及 高斯贝叶斯模型，决策树\n",
    "2.自己实现逻辑回归，SVM，以及朴素贝叶斯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "\n",
    "#数据归一化处理\n",
    "def data_nomalization(x):\n",
    "    std = np.std(x,axis=0) #标准差\n",
    "    mean = np.mean(x,axis=0) #均值\n",
    "    return ((x-mean)/std,mean,std)\n",
    "    \n",
    "# 数据处理\n",
    "def data_handle():\n",
    "    csv_data = pd.read_csv(\"employees_dataset.csv\")\n",
    "    # 将学历数值化。1:本科 2：硕士 3:博士 0：其它\n",
    "    csv_data[\"degree_cleaned\"] = np.where(csv_data[\"degree\"] == \"bachelor\", 1,\n",
    "                                          np.where(csv_data[\"degree\"] == 'master', 2,\n",
    "                                                   np.where(csv_data[\"degree\"] == \"phd\", 3, 0)))\n",
    "\n",
    "    # 读取世界前500名校，用于做学校清洗，是世界前500标记为1，不是标记为0\n",
    "    # 学校排名数据从网上爬下来的\n",
    "    famous_school_data = pd.read_csv(\"school.csv\")\n",
    "    school_name_list = famous_school_data[\"1\"]\n",
    "    school_name_list = np.array(school_name_list[1:], dtype=\"str\")\n",
    "    lower_name_list = [item.lower() for item in school_name_list]\n",
    "    edulist = np.array(csv_data[\"education\"])\n",
    "    edu_cleaned_list = []\n",
    "    for edu in edulist:\n",
    "        if edu in lower_name_list:\n",
    "            edu_cleaned_list.append(1)\n",
    "        else:\n",
    "            edu_cleaned_list.append(0)\n",
    "    csv_data[\"education_cleaned\"] = edu_cleaned_list\n",
    "\n",
    "    # 技能清洗，用16bit表示16种技能，每一位代表一种技能，有则为1，无则为0\n",
    "    skill_clean_ret = skill_clean(np.array(csv_data[\"skills\"]))\n",
    "    csv_data[\"skill_cleaned\"] = skill_clean_ret\n",
    "\n",
    "    #职位清洗\n",
    "    csv_data[\"position_cleaned\"] = np.where(csv_data[\"position\"] == \"dev\",1,\n",
    "                                               np.where(csv_data[\"position\"] == \"manager\",2,3))\n",
    "    return csv_data\n",
    "    \n",
    "\n",
    "def skill_clean(skills_list):\n",
    "    skill_filter = [\"c\", \"c++\", \"java,j2ee\", \"ios,objective-c,swift\", \"sql,database\", \"python\", \"linux\",\n",
    "                    \"software engineering,project management\", \"team building\", \"customer service\",\n",
    "                    \"test management,test plan\", \"test,test automation\", \"qtp\", \"quality assurance\", \"shell,script\"]\n",
    "    skill_clean_ret = []\n",
    "    for skills in skills_list:\n",
    "        ret = 0\n",
    "        skill_list = skills.split(\";\")\n",
    "        # 取前5种技能\n",
    "        skill_list = skill_list[:5]\n",
    "        for skill in skill_list:\n",
    "            skill = skill.replace('+', '\\+')\n",
    "            for index, k in enumerate(skill_filter):\n",
    "                if re.search(skill, k) != None:\n",
    "                    ret += int(math.pow(2, index+1))\n",
    "                    break\n",
    "        skill_clean_ret.append(np.log2(ret+1)) #取2为底的对数，降低数据集。效果会比不取对数好很多\n",
    "    return skill_clean_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logic_predict(train_x,train_y,test_x,test_y):\n",
    "    classifier = LogisticRegression(solver='lbfgs',multi_class='ovr',max_iter=500)\n",
    "    classifier.fit(X=train_x,y=train_y.astype(int))\n",
    "    y_predict = classifier.predict(test_x)\n",
    "\n",
    "    print(\"=== 逻辑回归 =====\")\n",
    "    judge_model(test_y,y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用SVM\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def svm_predict(train_x, train_y, test_x, test_y):\n",
    "    print(\"====== SVC =====\")\n",
    "    clf = SVC(gamma='auto', C=1, decision_function_shape='ovo')\n",
    "    clf.fit(train_x, train_y.astype(int))\n",
    "    print(\"====== 测试集 =====\")\n",
    "    predict_y = clf.predict(test_x)\n",
    "    judge_model(test_y, predict_y)\n",
    "    print(\"====== 训练集 =====\")\n",
    "    predict_y = clf.predict(train_x)\n",
    "    judge_model(train_y, predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用高斯贝叶斯\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def bayes_predict(train_x, train_y, test_x, test_y):\n",
    "    gsnb = GaussianNB()\n",
    "    gsnb.fit(train_x,train_y.astype(int))\n",
    "    predict_y = gsnb.predict(test_x)\n",
    "    print(\"=== 高斯贝叶斯 ====\")\n",
    "    judge_model(test_y,predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据精准率，召回率，FlScore评判模型\n",
    "def judge_model(real, predict):\n",
    "    # 正确率只是想计算一下看下\n",
    "    correct_num = np.sum((real-predict) == 0)\n",
    "    total = np.size(predict, axis=0)\n",
    "    rate = correct_num/total\n",
    "    print((\"正确率为:\"+repr(rate)))\n",
    "\n",
    "    print('=======')\n",
    "    print(\"实际值：\")\n",
    "    print(real)\n",
    "    print(\"预测值：\")\n",
    "    print(predict)\n",
    "    # 从real中获取测试的所有类的取值\n",
    "    class_list = set(real)\n",
    "    print('=======')\n",
    "    print((\"预测的类别有：\"+repr(class_list)))\n",
    "\n",
    "    # 计算每个类的精准率，召回率，FlScore\n",
    "    for c in class_list:\n",
    "        n_pred_c = np.sum(predict == c)\n",
    "        n_real_c = np.sum(real == c)\n",
    "        c_list = np.array([c]*total)\n",
    "        TP = np.sum((predict == c_list) & (\n",
    "            real == c_list) == True)  # 实际为c类，预测也为c类\n",
    "        #FP = np.sum(predict == c ) -TP  #实际不是c类，但被预测为c类\n",
    "#         FN = np.sum( real == c ) - TP #实际是c类，但被预测为其它类\n",
    "        print((\"TP:\"+str(TP)+\",pred_c:\"+str(n_pred_c)+\",real_c:\"+str(n_real_c)))\n",
    "\n",
    "        # 精准率\n",
    "        precision = recall = flscore = 0\n",
    "        if n_pred_c != 0:\n",
    "            precision = TP/n_pred_c\n",
    "        else:\n",
    "            precision = 0\n",
    "        # 召回率\n",
    "        if n_real_c != 0:\n",
    "            recall = float(TP)/n_real_c\n",
    "        else:\n",
    "            recall = 0\n",
    "\n",
    "        if (precision + recall) != 0:\n",
    "            flscore = 2*(precision * recall)/(precision + recall)\n",
    "        else:\n",
    "            flscore = 0\n",
    "\n",
    "        print((\"预测\"+repr(c)+\":\"))\n",
    "        print((\"精准率：\"+str(precision)+\",召回率：\" +\n",
    "               str(recall)+\",flscore:\"+repr(flscore)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 逻辑回归 =====\n",
      "正确率为:0.6666666666666666\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 2 1 2 1 1 2 1 2 2]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:6,pred_c:7,real_c:8\n",
      "预测1:\n",
      "精准率：0.8571428571428571,召回率：0.75,flscore:0.7999999999999999\n",
      "TP:2,pred_c:5,real_c:3\n",
      "预测2:\n",
      "精准率：0.4,召回率：0.6666666666666666,flscore:0.5\n",
      "TP:0,pred_c:0,real_c:1\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      "====== SVC =====\n",
      "====== 测试集 =====\n",
      "正确率为:0.75\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 2 1 1 3 1 2 3]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:7,pred_c:8,real_c:8\n",
      "预测1:\n",
      "精准率：0.875,召回率：0.875,flscore:0.875\n",
      "TP:1,pred_c:2,real_c:3\n",
      "预测2:\n",
      "精准率：0.5,召回率：0.3333333333333333,flscore:0.4\n",
      "TP:1,pred_c:2,real_c:1\n",
      "预测3:\n",
      "精准率：0.5,召回率：1.0,flscore:0.6666666666666666\n",
      "====== 训练集 =====\n",
      "正确率为:0.8666666666666667\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 3 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 3 3 3 1 1 3 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:34,pred_c:39,real_c:37\n",
      "预测1:\n",
      "精准率：0.8717948717948718,召回率：0.918918918918919,flscore:0.8947368421052632\n",
      "TP:14,pred_c:16,real_c:16\n",
      "预测2:\n",
      "精准率：0.875,召回率：0.875,flscore:0.875\n",
      "TP:4,pred_c:5,real_c:7\n",
      "预测3:\n",
      "精准率：0.8,召回率：0.5714285714285714,flscore:0.6666666666666666\n",
      "=== 高斯贝叶斯 ====\n",
      "正确率为:0.5833333333333334\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 3 2 2 2 1 2 2 2 2]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:4,pred_c:4,real_c:8\n",
      "预测1:\n",
      "精准率：1.0,召回率：0.5,flscore:0.6666666666666666\n",
      "TP:3,pred_c:7,real_c:3\n",
      "预测2:\n",
      "精准率：0.42857142857142855,召回率：1.0,flscore:0.6\n",
      "TP:0,pred_c:1,real_c:1\n",
      "预测3:\n",
      "精准率：0.0,召回率：0.0,flscore:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib as plt\n",
    "def main():\n",
    "    data = data_handle()\n",
    "    #将数据分为训练集和测试集\n",
    "    #train,test = train_test_split(data,test_size=0.1,random_state=int(time.time()))\n",
    "    \n",
    "        #因为样本不均匀，随机抽取可能会使某一类不在训练集或全在训练集，所以手动选择\n",
    "    test = data[::6]\n",
    "    train = data.append(test).drop_duplicates(keep = False)\n",
    "\n",
    "    train = np.array(train)\n",
    "    train_x = train[:,5:8]\n",
    "    train_y = train[:,8]\n",
    "    \n",
    "    test = np.array(test)\n",
    "    test_x = test[:,5:8]\n",
    "    test_y = test[:,8]\n",
    "\n",
    "\n",
    "    logic_predict(train_x,train_y,test_x,test_y)\n",
    "    svm_predict(train_x,train_y,test_x,test_y)\n",
    "\n",
    "    bayes_predict(train_x,train_y,test_x,test_y)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己编写逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的大小:61\n",
      "测试集的大小:11\n",
      "=== 逻辑回归 =====\n",
      "正确率为:0.7272727272727273\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 1 1 2 2]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:7,pred_c:9,real_c:7\n",
      "预测1:\n",
      "精准率：0.7777777777777778,召回率：1.0,flscore:0.8750000000000001\n",
      "TP:1,pred_c:2,real_c:3\n",
      "预测2:\n",
      "精准率：0.5,召回率：0.3333333333333333,flscore:0.4\n",
      "TP:0,pred_c:0,real_c:1\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      " ===== 开始 训练 ====== \n",
      "(61, 7)\n",
      "(7,)\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "最终得到的theta为：array([ 2.70624142e-04, -3.26531035e-01, -1.94051969e-01, -1.40964975e+00,\n",
      "        7.47952655e-02,  2.62301630e-01,  4.05328320e-01])\n",
      "1的最终的损失为:0.4985696814176488\n",
      "==== end  ======== \n",
      "(7,)\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "最终得到的theta为：array([-4.17354935e-04,  2.51900671e-01,  8.58543212e-02,  1.22508662e+00,\n",
      "       -1.00263001e-03, -7.00288113e-01, -7.00551872e-01])\n",
      "2的最终的损失为:0.4575282335037266\n",
      "==== end  ======== \n",
      "(7,)\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "最终得到的theta为：array([-9.31564581e-04, -3.49978623e-01,  1.12247952e-01,  7.37721044e-01,\n",
      "       -8.70904772e-01,  1.04799090e-01, -1.52331369e+00])\n",
      "3的最终的损失为:0.31120570305652495\n",
      "==== end  ======== \n",
      "========训练集验证 ：=======\n",
      "类1的损失为：0.5189\n",
      "类2的损失为：0.4883\n",
      "类3的损失为：0.3549\n",
      "正确率为:0.5901639344262295\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 2 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 2 0 2 1 2 0 1 2 0 2 0 0 0 1 0 0 0 0 0 1 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:31,pred_c:37,real_c:38\n",
      "预测1:\n",
      "精准率：0.8378378378378378,召回率：0.8157894736842105,flscore:0.8266666666666665\n",
      "TP:5,pred_c:6,real_c:16\n",
      "预测2:\n",
      "精准率：0.8333333333333334,召回率：0.3125,flscore:0.45454545454545453\n",
      "TP:0,pred_c:0,real_c:7\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      "======= 测试集验证 ：======\n",
      "类1的损失为：0.6014\n",
      "类2的损失为：0.6140\n",
      "类3的损失为：0.4050\n",
      "正确率为:0.5454545454545454\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[0 1 1 1 1 0 1 0 0 2 0]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:5,pred_c:5,real_c:7\n",
      "预测1:\n",
      "精准率：1.0,召回率：0.7142857142857143,flscore:0.8333333333333333\n",
      "TP:1,pred_c:1,real_c:3\n",
      "预测2:\n",
      "精准率：1.0,召回率：0.3333333333333333,flscore:0.5\n",
      "TP:0,pred_c:0,real_c:1\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 自己实现逻辑回归\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# 定义梯度下降\n",
    "# X_set 变量集\n",
    "# y_set 结果集\n",
    "# alpha 学习速率\n",
    "\n",
    "\n",
    "def gradient_descent(X_set, y_set, alpha, theta,lamda):\n",
    "    #print(\"==== 梯度 下降 ===== \")\n",
    "    m = np.size(X_set, axis=0)\n",
    "    regular_theta = theta\n",
    "    regular_theta[0] = 0\n",
    "    h_x =1/(1+np.e** -np.dot(X_set , theta))\n",
    "    derivative_sum = np.dot(X_set.transpose(), h_x - y_set)\n",
    "    theta = theta - alpha*(derivative_sum/m + (lamda/m)*regular_theta)\n",
    "    #theta = theta - alpha * derivative_sum/m\n",
    "    return theta\n",
    "\n",
    "# 定义代价函数\n",
    "\n",
    "\n",
    "def cost_function(X_set, y, theta,lamda):\n",
    "   # print(\"==== 代价函数  计算损失==== \")\n",
    "    #cost = -1/m(sum(ylog(h(x) + (1-y)log(1-h(x)))))\n",
    "    m = np.size(X_set, axis=0)  # 样本个数\n",
    "    h_x =1/(1+np.e** -np.dot(X_set , theta))\n",
    "    regular_theta  = theta.copy()\n",
    "    regular_theta[0] = 0\n",
    "    cost = np.sum (y*np.log(h_x) + (1-y)*np.log(1-h_x))/-m+(lamda/(2*m))*np.sum(np.square(regular_theta))\n",
    "    #cost = np.sum (y*np.log(h_x) + (1-y)*np.log(1-h_x))/-m\n",
    "    return cost\n",
    "\n",
    "# 定义训练方法\n",
    "def train(X_train, y_train):\n",
    "    print(\" ===== 开始 训练 ====== \")\n",
    "    \n",
    "\n",
    "    #增加多项式\n",
    "    x3s = X_train[:,2]*X_train[:,2]\n",
    "    x1s = X_train[:,0]*X_train[:,0]\n",
    "    x2s = X_train[:,1]*X_train[:,1]\n",
    "\n",
    "\n",
    "    X_train = np.hstack((X_train,x1s.reshape(-1,1)))\n",
    "    X_train = np.hstack((X_train,x3s.reshape(-1,1)))\n",
    "    X_train = np.hstack((X_train,x2s.reshape(-1,1)))\n",
    "\n",
    "\n",
    "    # 初始化第0列全为1，作为theta_0的变量\n",
    "    X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "    dimention = np.size(X_train, axis=1)  # X的维度\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    #获取所有y的取值,然后将y的结果集拆分成n个结果集，每个结果集判断一种y的取值，是为1，不是为0\n",
    "    y_set = set(y_train)\n",
    "    \n",
    "    theta_dict  = {}\n",
    "    for y in y_set :\n",
    "        y_train_new = np.where (y_train== y , 1 , 0)\n",
    "        theta = np.zeros(dimention) # 初始化theta变量全为0\n",
    "        print(theta.shape)\n",
    "        print(theta)\n",
    "        \n",
    "        #训练单个分类模型\n",
    "        theta = sub_train ( X_train , y_train_new,theta,y)\n",
    "        theta_dict[y] = theta\n",
    "    return theta_dict\n",
    "        \n",
    "def sub_train ( X_train, y_train , theta,y) :\n",
    "        num_iters = 500  # 迭代次数\n",
    "        alpha = 0.05\n",
    "        cost_plot = list()\n",
    "        iteration = range(1, num_iters)\n",
    "        lamda = 0.0     \n",
    "        for i in iteration:\n",
    "            theta = gradient_descent(X_train, y_train, alpha, theta,lamda)\n",
    "            # 计算损失\n",
    "            cost = cost_function(X_train, y_train, theta,lamda)\n",
    "            cost_plot.append(cost)\n",
    "            #print(\"第\"+repr(i)+\"迭代之后的 损失为：\" + repr(cost))\n",
    "\n",
    "    # 画出损失函数图\n",
    "        plt.xlim(0, num_iters)\n",
    "        plt.plot(iteration, cost_plot)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"cost\")\n",
    "    \n",
    "        print ( \"最终得到的theta为：\"+ repr(theta))\n",
    "        print (repr(y)+\"的最终的损失为:\"+repr(cost))\n",
    "        print(\"==== end  ======== \")\n",
    "        return theta\n",
    "    \n",
    "#预测\n",
    "def predicate ( X_set , theta_dict) :\n",
    "    m = np.size(X_set, axis=0)  # 样本个数\n",
    "    x3s = X_set[:,2]*X_set[:,2]\n",
    "    x1s = X_set[:,0]*X_set[:,0]\n",
    "    x2s = X_set[:,1]*X_set[:,1]\n",
    "    X_set = np.hstack((X_set,x1s.reshape(-1,1)))\n",
    "    X_set = np.hstack((X_set,x3s.reshape(-1,1)))\n",
    "    X_set = np.hstack((X_set,x2s.reshape(-1,1)))\n",
    "\n",
    "\n",
    "    X_set = np.insert ( X_set , 0 , 1 , axis = 1)\n",
    "    \n",
    "    last_y_pred = np.zeros(m) #默认初始化预测值为0\n",
    "    #循环遍历每个取值的概率模型，取取值最大的那个，再判断最大的那个的概率是否大于0.5\n",
    "    #这里是假定取值y_key 为整数\n",
    "    for y_key in theta_dict :\n",
    "        theta = theta_dict.get(y_key)\n",
    "        y_pred = 1 / (1 + np.e ** -np.dot(X_set, theta))\n",
    "        cost = cost_function(X_set,y_pred,theta,0.0)\n",
    "        print(\"类%d的损失为：%.4f\" %(y_key ,cost))\n",
    "        last_y_pred = np.where(y_pred > (last_y_pred-last_y_pred.astype(int)) , y_key + y_pred,last_y_pred)\n",
    "    last_y_pred = np.where ( last_y_pred - last_y_pred.astype(int) > 0.5, last_y_pred.astype(int), 0)\n",
    "    \n",
    "    \n",
    "    return last_y_pred\n",
    "    \n",
    "# 定义主函数\n",
    "def my_main():\n",
    "    data = data_handle()\n",
    "     # 将数据分为训练集和测试集\n",
    "#     train_set, test_set  = train_test_split(\n",
    "#         data, test_size=0.1, random_state=int(time.time()))\n",
    "    #因为样本不均匀，随机抽取可能会使某一类不在训练集或全在训练集，所以手动选择\n",
    "    test_set = data[::7]\n",
    "    train_set = data.append(test_set).drop_duplicates(keep = False)\n",
    "    \n",
    "\n",
    "    train_set = np.array(train_set)\n",
    "    train_X = train_set [:, 5:8]\n",
    "    train_y = train_set [:, 8]\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_X = test_set[:, 5:8]\n",
    "    test_y = test_set[:, 8]\n",
    "    \n",
    "    print(\"训练集的大小:\"+repr(np.size(train_X,axis=0)))\n",
    "    print(\"测试集的大小:\"+repr(np.size(test_X,axis=0)))\n",
    "    logic_predict(train_X,train_y,test_X,test_y)\n",
    "    \n",
    "    train_X,mean,std = data_nomalization(train_X.astype(float))\n",
    "    test_X = (test_X - std)/mean\n",
    "\n",
    "    theta = train (train_X.astype(float) , train_y)\n",
    "    \n",
    "    print(\"========训练集验证 ：=======\")\n",
    "    y_pred = predicate(train_X.astype(float) , theta)\n",
    "    judge_model (train_y , y_pred )\n",
    "\n",
    "    \n",
    "    print(\"======= 测试集验证 ：======\")\n",
    "    y_pred = predicate(test_X.astype(float) , theta)\n",
    "    judge_model (test_y , y_pred )\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "my_main ( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己实现贝叶斯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 高斯贝叶斯 ====\n",
      "正确率为:1.0\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:7,pred_c:7,real_c:7\n",
      "预测1:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n",
      "TP:3,pred_c:3,real_c:3\n",
      "预测2:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n",
      "TP:1,pred_c:1,real_c:1\n",
      "预测3:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n",
      "==== bayes  train =======\n",
      "样本总个数:61\n",
      "训练集验证 ：\n",
      "===== 预测  ====== \n",
      "{1: 0.6229508196721312, 2: 0.26229508196721313, 3: 0.11475409836065574}\n",
      "正确率为:0.6721311475409836\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[1 2 1 1 1 2 2 1 1 2 1 1 1 2 1 3 1 1 2 1 2 1 1 2 2 2 2 3 1 1 2 1 1 1 2 1 1\n",
      " 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 2 1 2]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:23,pred_c:25,real_c:38\n",
      "预测1:\n",
      "精准率：0.92,召回率：0.6052631578947368,flscore:0.7301587301587301\n",
      "TP:14,pred_c:29,real_c:16\n",
      "预测2:\n",
      "精准率：0.4827586206896552,召回率：0.875,flscore:0.6222222222222222\n",
      "TP:4,pred_c:7,real_c:7\n",
      "预测3:\n",
      "精准率：0.5714285714285714,召回率：0.5714285714285714,flscore:0.5714285714285714\n",
      "测试集验证 ：\n",
      "===== 预测  ====== \n",
      "{1: 0.6229508196721312, 2: 0.26229508196721313, 3: 0.11475409836065574}\n",
      "正确率为:0.9090909090909091\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 2 1 2 2 2 3]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:6,pred_c:6,real_c:7\n",
      "预测1:\n",
      "精准率：1.0,召回率：0.8571428571428571,flscore:0.923076923076923\n",
      "TP:3,pred_c:4,real_c:3\n",
      "预测2:\n",
      "精准率：0.75,召回率：1.0,flscore:0.8571428571428571\n",
      "TP:1,pred_c:1,real_c:1\n",
      "预测3:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n"
     ]
    }
   ],
   "source": [
    "# 自己实现贝叶斯分类模型\n",
    "# 假设P(Xi|C) 符合高斯分布\n",
    "\n",
    "\n",
    "def bayes_train(X_train, y_train):\n",
    "    print(\"==== bayes  train =======\")\n",
    "    # 由贝叶斯的似然函数可知道，假设k个类C，特征值X有n个，则theta矩阵为 k*n或n*k\n",
    "    # 因为为高斯分布，所以每一个theta有两个参数值 u和sigma^2\n",
    "\n",
    "    class_y = set(y_train)  # 统计分类\n",
    "    class_cnt = len(class_y)  # 分类的个数\n",
    "    feature_cnt = np.size(X_train, axis=1)  # X的维度即特征值的个数\n",
    "    \n",
    "    m = np.size ( X_train , axis=0) #样本个数\n",
    "    print(\"样本总个数:\"+repr(m))\n",
    "    class_theta_map = {} #类与theta参数的对应关系\n",
    "    class_p_map = {}  #类的概率\n",
    "    for y in class_y:\n",
    "        sample_rows = np.where(y_train==y)  # 返回y_train结果集中为y类的行号\n",
    "\n",
    "        # 根据行号取出对应的特征集\n",
    "        X_sample = X_train[sample_rows]\n",
    "        mc = np.size(X_sample, axis=0)\n",
    "\n",
    "        # 求均值\n",
    "        oneclass_mean_matrix = np.sum(X_sample, axis=0)/mc  # 得到一个类别在各个特征值上的均值\n",
    "\n",
    "\n",
    "        # 求sigma^2\n",
    "        oneclass_sigma_square_matrix = np.sum(\n",
    "            np.square(X_sample - oneclass_mean_matrix), axis=0)/mc\n",
    "        #合并两个参数 得到一个类似[[std_1,sigma_1],[std_2,sigma_2] ...]\n",
    "        oneclass_theta = np.array(np.vstack((oneclass_mean_matrix,oneclass_sigma_square_matrix)))\n",
    "        class_theta_map [y] = oneclass_theta\n",
    "        \n",
    "        P_c = mc / m #C类出现的概率\n",
    "        class_p_map[y] = P_c\n",
    "        \n",
    "    return class_theta_map,class_p_map\n",
    "\n",
    "#预测\n",
    "def predicate(X_set , class_theta_map , class_p_map) :\n",
    "    print(\"===== 预测  ====== \")\n",
    "    m = np.size(X_set, axis=0)\n",
    "    result = pd.DataFrame(np.zeros((m,2)),columns=[\"p\",\"class\"]) #初始\n",
    "    #循环计算每个类的概率,去掉1/Z\n",
    "    \n",
    "    print(class_p_map)\n",
    "    for c in class_theta_map :\n",
    "        P_c = class_p_map[c]\n",
    "        theta = class_theta_map[c]\n",
    "        mean = theta[0]\n",
    "        sigma_square = theta[1]\n",
    "        sigma = np.sqrt(sigma_square)\n",
    "        const_num = np.sqrt(2*np.pi)\n",
    "        #print(const_num)\n",
    "        P_x_c = np.prod ((1/(const_num*sigma))*(np.e ** -np.square(X_set - mean)/(2*sigma_square)),axis=1)\n",
    "        #print(P_x_c)\n",
    "        P_c_x = P_c*P_x_c.astype(float)\n",
    "     \n",
    "        result[\"class\"] = np.where(result[\"p\"] < P_c_x, c, result[\"class\"]).astype(int)\n",
    "        result[\"p\"] = np.where(result[\"p\"] < P_c_x, P_c_x, result[\"p\"]) #取概率大的那个\n",
    "    return np.array(result[\"class\"].astype(int) )    \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# 定义主函数\n",
    "def bayes_main():\n",
    "    data = data_handle()\n",
    "    # 将数据分为训练集和测试集\n",
    "      #因为样本不均匀，随机抽取可能会使某一类不在训练集或全在训练集，所以手动选择\n",
    "    test_set = data[::7]\n",
    "    train_set = data.append(test_set).drop_duplicates(keep = False)\n",
    "\n",
    "    train_set = np.array(train_set)\n",
    "    train_X = train_set [:, 5:8]\n",
    "    train_y = train_set [:, 8]\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_X = test_set[:, 5:8]\n",
    "    test_y = test_set[:, 8]\n",
    "    \n",
    "    bayes_predict(train_X,train_y,test_X,test_y)\n",
    "    \n",
    "    theta,class_p = bayes_train (train_X.astype(float) , train_y)\n",
    "    \n",
    "    print(\"训练集验证 ：\")\n",
    "    y_pred = predicate(train_X , theta,class_p)\n",
    "    judge_model (train_y , y_pred )\n",
    "    \n",
    "    print(\"测试集验证 ：\")\n",
    "    y_pred = predicate(test_X , theta,class_p)\n",
    "    judge_model (test_y , y_pred )\n",
    "    \n",
    "    \n",
    "bayes_main ( )        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己实现SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     41
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 单个子SVM\n",
    "class SubSVM:\n",
    "    # 成员变量\n",
    "    w = 0  # 最大超平面的w向量值\n",
    "    b = 0  # 最大超平面的b值\n",
    "    category = 0\n",
    "    # 构造函数，初始化，传入类别\n",
    "\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "        matplotlib.use('TkAgg')\n",
    "\n",
    "    # 训练\n",
    "    def train(self, X_train, y_train):\n",
    "        m = np.size(X_train, axis=0)  # 样本个数\n",
    "        # 使用SMO算法最优化对偶问题，求得最优的alpha\n",
    "        C = 1.0\n",
    "        max_iter = 100\n",
    "        optimize_alpha,b = self.smo_simple(X_train, y_train, C, max_iter)\n",
    "        # 求w\n",
    "        self.w = np.dot(X_train.transpose(), optimize_alpha*y_train)\n",
    "        # 求b，由KKT条件得：(1-y(wx+b)) = 0。求所有支持向量的均值\n",
    "        self.b = np.sum(y_train - np.dot(X_train, self.w))/m\n",
    "        print(\"==== w === \")\n",
    "        print(self.w)\n",
    "        print(\"====== b :\"+repr(self.b))\n",
    "\n",
    "    # SMO算法\n",
    "    def smo_simple(self, X_train, y_train, C, max_iter):\n",
    "        print(\"====== SMO ===== \")\n",
    "        m, n = X_train.shape\n",
    "        #alpha = np.zeros(m).astype(float)  # 初始化alpha\n",
    "        alpha = np.ones(m)\n",
    "        b = 0\n",
    "        iters = 0\n",
    "        cost_plot = list()\n",
    "        iteration = range(1,max_iter+1)\n",
    "        while iters < max_iter:\n",
    "            iters += 1\n",
    "            for i in range(0, m):\n",
    "                # 求f(xi)的值\n",
    "                fxi = np.sum(alpha * y_train *\n",
    "                             np.dot(X_train, X_train[i].transpose()))+b\n",
    "                # 求实际值与预测值之间的差\n",
    "                Ei = fxi - y_train[i]\n",
    "\n",
    "                # 随机选取第二个参数j， j!=i\n",
    "                j = self.select_j_rand(i, m)\n",
    "                fxj = np.sum(alpha * y_train *\n",
    "                             np.dot(X_train, X_train[j].transpose()))+b\n",
    "                Ej = fxj - y_train[j]\n",
    "\n",
    "                # Kii+Kjj-2Kij\n",
    "                eta = np.dot(X_train[i], X_train[i].transpose())+np.dot(X_train[j],\n",
    "                                                                        X_train[j].transpose()) - 2*np.dot(X_train[i], X_train[j].transpose())\n",
    "                \n",
    "                if(eta == 0) :\n",
    "                    continue\n",
    "                alpha_j_new = (\n",
    "                    y_train[j]*(Ei - Ej))/eta + alpha[j]\n",
    "                \n",
    "                alpha_i_old = alpha[i].copy()\n",
    "                alpha_j_old = alpha[j].copy()\n",
    "                \n",
    "                # 根据约束条件裁剪alpha_j_new\n",
    "                alpha_j_new,L,H = self.clipped_alpha(\n",
    "                    alpha_j_new, y_train[i], y_train[j], alpha[i], alpha[j], C)\n",
    "                if ( L == H ) :\n",
    "                  #  print(\"第\"+repr(iters)+\"次循环:L=H=\"+repr(L)+\",i=\"+repr(i))\n",
    "                    continue\n",
    "        \n",
    "          \n",
    "      \n",
    "                alpha[j] = alpha_j_new\n",
    "        \n",
    "                if(abs(alpha_j_new - alpha_j_old) < 0.0001) :\n",
    "                    #print(\"第\"+repr(iters)+\"次循环,改变很小，无需优化\"+repr(j))\n",
    "                    continue\n",
    "                # 根据alpha_j 求alpha_i\n",
    "                alpha_i_new = alpha[i] + (alpha_j_old - alpha_j_new)*y_train[i]*y_train[j]\n",
    "\n",
    "              # 更新alpha\n",
    "                alpha[i] = alpha_i_new\n",
    "                \n",
    "\n",
    "                # 更新b\n",
    "                b1 = (alpha_i_old-alpha[i])*y_train[i]*np.dot(X_train[i], X_train[i].transpose())+(\n",
    "                    alpha_j_old-alpha[j])*y_train[j] * np.dot(X_train[j], X_train[i].transpose())+b-Ei\n",
    "\n",
    "                b2 = (alpha_i_old-alpha[i])*y_train[i]*np.dot(X_train[i], X_train[j].transpose())+(\n",
    "                    alpha_j_old-alpha[j])*y_train[j] * np.dot(X_train[j], X_train[j].transpose())+b-Ej\n",
    "\n",
    "                if 0< alpha[i] and C > alpha[i]:\n",
    "                    b = b1\n",
    "                elif 0< alpha[j] and C > alpha[j]:\n",
    "                    b = b2\n",
    "                else:\n",
    "                    b = (b1+b2)/2\n",
    "            #计算损失\n",
    "            cost = self.cost_function(X_train,y_train,b,alpha)\n",
    "            cost_plot.append(cost)\n",
    "        print(\"==== 最终损失为：\"+repr(cost))\n",
    "        # 画出损失函数图\n",
    "        plt.xlim(0, max_iter)\n",
    "        plt.plot(iteration,cost_plot)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"cost\")\n",
    "\n",
    "    \n",
    "        return alpha,b\n",
    "\n",
    "    # 随机选择第二个参数的index j\n",
    "    def select_j_rand(self, i, m):\n",
    "        j = i\n",
    "        while (j == i):\n",
    "            j = random.randint(0, m-1)\n",
    "        return j\n",
    "\n",
    "    # 裁剪alpha\n",
    "    def clipped_alpha(self, alpha_new_x, yi, yj, alpha_i, alpha_j, C):\n",
    "        L = H = 0\n",
    "        if yi != yj:           \n",
    "            k = alpha_j - alpha_i\n",
    "            # 确定下界\n",
    "            L = max(k, 0)\n",
    "            # 确定上界\n",
    "            H = min(C, C+k)\n",
    "        else:\n",
    "            k = alpha_i+alpha_j\n",
    "            L = max(0, k - C)\n",
    "            H = min(k, C)\n",
    "        # alpha_new 必须介于L和H之间\n",
    "        alpha_new_x = max(L, alpha_new_x)\n",
    "        alpha_new_x = min(H, alpha_new_x)\n",
    "                \n",
    "        return alpha_new_x,L,H\n",
    "\n",
    "    #计算损失\n",
    "    def cost_function(self , X_train, y_train, b,alpha):\n",
    "        m ,n = X_train.shape\n",
    "        cost =  np.sum(np.square(np.dot( np.dot(X_train, X_train.transpose()), alpha*y_train) + b - y_train))/m\n",
    "        return cost\n",
    "        \n",
    "    # 决策\n",
    "    def sign(self, X):\n",
    "        f_x =  np.dot(X, self.w) + self.b\n",
    "        # 决策函数决策，返回1，0， -1 预测值。1 表示正类。即属于self.category\n",
    "        sign_fx = np.where(f_x < 0, -1, np.where(f_x == 0, 0, 1))\n",
    "        return sign_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== SVC =====\n",
      "====== 测试集 =====\n",
      "正确率为:0.9090909090909091\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 2 2 2 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:7,pred_c:8,real_c:7\n",
      "预测1:\n",
      "精准率：0.875,召回率：1.0,flscore:0.9333333333333333\n",
      "TP:3,pred_c:3,real_c:3\n",
      "预测2:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n",
      "TP:0,pred_c:0,real_c:1\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      "====== 训练集 =====\n",
      "正确率为:0.8524590163934426\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 1 1 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:34,pred_c:38,real_c:38\n",
      "预测1:\n",
      "精准率：0.8947368421052632,召回率：0.8947368421052632,flscore:0.8947368421052632\n",
      "TP:14,pred_c:18,real_c:16\n",
      "预测2:\n",
      "精准率：0.7777777777777778,召回率：0.875,flscore:0.823529411764706\n",
      "TP:4,pred_c:5,real_c:7\n",
      "预测3:\n",
      "精准率：0.8,召回率：0.5714285714285714,flscore:0.6666666666666666\n",
      "====== 自己写的 SVM ======\n",
      "====== SMO ===== \n",
      "==== 最终损失为：103.9604234466903\n",
      "==== w === \n",
      "[17.          3.84765138  0.38057538]\n",
      "====== b :-33.121242351735084\n",
      "====== SMO ===== \n",
      "==== 最终损失为：52738.764322012874\n",
      "==== w === \n",
      "[-49.         -14.         -47.24618126]\n",
      "====== b :357.8294607572948\n",
      "====== SMO ===== \n",
      "==== 最终损失为：1556496.732386223\n",
      "==== w === \n",
      "[ -83.          -24.         -197.88662851]\n",
      "====== b :1271.9563406674827\n",
      "=======训练集验证 ：======\n",
      "===== 预测 ====== \n",
      "==== 预测 :1\n",
      "==== 预测 :2\n",
      "==== 预测 :3\n",
      "正确率为:0.47540983606557374\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[2 2 1 1 1 1 1 1 1 3 2 2 1 1 2 3 1 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 1\n",
      " 2 1 1 1 3 1 3 1 1 3 1 1 1 3 1 1 3 1 1 3 3 1 1 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:27,pred_c:43,real_c:38\n",
      "预测1:\n",
      "精准率：0.627906976744186,召回率：0.7105263157894737,flscore:0.6666666666666666\n",
      "TP:0,pred_c:6,real_c:16\n",
      "预测2:\n",
      "精准率：0.0,召回率：0.0,flscore:0\n",
      "TP:2,pred_c:12,real_c:7\n",
      "预测3:\n",
      "精准率：0.16666666666666666,召回率：0.2857142857142857,flscore:0.2105263157894737\n",
      "======= 测试集验证 ：=======\n",
      "===== 预测 ====== \n",
      "==== 预测 :1\n",
      "==== 预测 :2\n",
      "==== 预测 :3\n",
      "正确率为:0.45454545454545453\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 2 1 1 1 2 3 3 1 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:5,pred_c:7,real_c:7\n",
      "预测1:\n",
      "精准率：0.7142857142857143,召回率：0.7142857142857143,flscore:0.7142857142857143\n",
      "TP:0,pred_c:2,real_c:3\n",
      "预测2:\n",
      "精准率：0.0,召回率：0.0,flscore:0\n",
      "TP:0,pred_c:2,real_c:1\n",
      "预测3:\n",
      "精准率：0.0,召回率：0.0,flscore:0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "# 线性可分SVM\n",
    "class LinearSVM:\n",
    "    model_list = list()  # 因为SVM是一个二分类，所以通过构造N-1个SubSVM来进行多分类\n",
    "\n",
    "    # 训练\n",
    "    def fit(self, X_train, y_train):\n",
    "        m = np.size(X_train, axis=0)  # 样本个数\n",
    "        # 获取所有y的取值,然后将y的结果集拆分成n个结果集，每个结果集判断一种y的取值，是为1，不是为-1\n",
    "        y_set = set(y_train)\n",
    "        for y in y_set:\n",
    "            y_sub_train = np.where(y_train == y, 1, -1)\n",
    "            subSVM = SubSVM(y)\n",
    "            subSVM.train(X_train, y_sub_train)\n",
    "            self.model_list.append(subSVM)\n",
    "\n",
    "     # 预测\n",
    "\n",
    "    def predict(self, X):\n",
    "        m = np.size(X, axis=0)  # 样本个数\n",
    "        y_pred = np.zeros(m)\n",
    "        print(\"===== 预测 ====== \")\n",
    "        # 让样本经过每一个模型进行预测\n",
    "        need_pred_X = X\n",
    "        result = pd.DataFrame(np.zeros((m,2)),columns=[\"p\",\"class\"]) #初始\\\n",
    "        unjudge_samples = np.array(range(0,m))\n",
    "        for index, model in enumerate(self.model_list):\n",
    "            \n",
    "            print(\"==== 预测 :\"+repr(model.category))\n",
    "            sign_x = model.sign(need_pred_X)\n",
    "#             result[\"class\"] = np.where(result[\"p\"] <= sign_x, model.category, result[\"class\"])\n",
    "#             result[\"p\"] = np.where(result[\"p\"] <= sign_x, sign_x, result[\"p\"])\n",
    "            \n",
    "            if  index == (len(self.model_list) - 1):\n",
    "                # 最后一个模型，无需再分类\n",
    "                y_pred[unjudge_samples] = model.category\n",
    "            else:\n",
    "                negtive_columns=np.where(sign_x != 1)\n",
    "                positive_columns=np.where(sign_x == 1)\n",
    "\n",
    "                need_pred_X = X[negtive_columns]\n",
    "                y_pred[unjudge_samples[positive_columns]] = model.category\n",
    "                unjudge_samples = unjudge_samples[negtive_columns]\n",
    "        return y_pred.astype(int)\n",
    "\n",
    "def my_svm_main() :\n",
    "    \n",
    "    data = data_handle()\n",
    "    # 将数据分为训练集和测试集\n",
    "      #因为样本不均匀，随机抽取可能会使某一类不在训练集或全在训练集，所以手动选择\n",
    "    test_set = data[::7]\n",
    "    train_set = data.append(test_set).drop_duplicates(keep = False)\n",
    "\n",
    "    train_set = np.array(train_set)\n",
    "    train_X = train_set [:, 5:8]\n",
    "    train_y = train_set [:, 8]\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_X = test_set[:, 5:8]\n",
    "    test_y = test_set[:, 8]\n",
    "    \n",
    "    svm_predict(train_X,train_y,test_X,test_y)\n",
    "#     train_X,mean,std = data_nomalization(train_X.astype(float))\n",
    "#     test_X = (test_X-mean)/std\n",
    "      \n",
    "    print(\"====== 自己写的 SVM ======\")\n",
    "    mysvm = LinearSVM()\n",
    "    mysvm.fit(train_X.astype(float), train_y)\n",
    "    \n",
    "    print(\"=======训练集验证 ：======\")\n",
    "    \n",
    "    y_pred = mysvm.predict(train_X)\n",
    "    judge_model (train_y , y_pred )\n",
    "    \n",
    "    print(\"======= 测试集验证 ：=======\")\n",
    "    y_pred = mysvm.predict(test_X)\n",
    "    judge_model (test_y , y_pred )\n",
    "    \n",
    "my_svm_main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
