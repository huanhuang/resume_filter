{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    简历过滤器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用提供的数据集训练一个模型，用来过滤求职者的简历；\n",
    "用来判断求职者是：不能被录取、适合当开发工程师、适合当测试工程师，还是适合当经理\n",
    "验证标准：精准率，召回率和 F1Score。\n",
    "\n",
    "1.尝试使用 sklearn 的 逻辑回归模型，SVM模型，以及 高斯贝叶斯模型，决策树\n",
    "2.自己实现逻辑回归，SVM，以及朴素贝叶斯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "\n",
    "#数据归一化处理\n",
    "def data_nomalization(x):\n",
    "    std = np.std(x,axis=0) #标准差\n",
    "    mean = np.mean(x,axis=0) #均值\n",
    "    return ((x-mean)/std,mean,std)\n",
    "    \n",
    "# 数据处理\n",
    "def data_handle():\n",
    "    csv_data = pd.read_csv(\"employees_dataset.csv\")\n",
    "    # 将学历数值化。1:本科 2：硕士 3:博士 0：其它\n",
    "    csv_data[\"degree_cleaned\"] = np.where(csv_data[\"degree\"] == \"bachelor\", 1,\n",
    "                                          np.where(csv_data[\"degree\"] == 'master', 2,\n",
    "                                                   np.where(csv_data[\"degree\"] == \"phd\", 3, 0)))\n",
    "\n",
    "    # 读取世界前500名校，用于做学校清洗，是世界前500标记为1，不是标记为0\n",
    "    # 学校排名数据从网上爬下来的\n",
    "    famous_school_data = pd.read_csv(\"school.csv\")\n",
    "    school_name_list = famous_school_data[\"1\"]\n",
    "    school_name_list = np.array(school_name_list[1:], dtype=\"str\")\n",
    "    lower_name_list = [item.lower() for item in school_name_list]\n",
    "    edulist = np.array(csv_data[\"education\"])\n",
    "    edu_cleaned_list = []\n",
    "    for edu in edulist:\n",
    "        if edu in lower_name_list:\n",
    "            edu_cleaned_list.append(1)\n",
    "        else:\n",
    "            edu_cleaned_list.append(0)\n",
    "    csv_data[\"education_cleaned\"] = edu_cleaned_list\n",
    "\n",
    "    # 技能清洗，用16bit表示16种技能，每一位代表一种技能，有则为1，无则为0\n",
    "    skill_clean_ret = skill_clean(np.array(csv_data[\"skills\"]))\n",
    "    csv_data[\"skill_cleaned\"] = skill_clean_ret\n",
    "\n",
    "    #职位清洗\n",
    "    csv_data[\"position_cleaned\"] = np.where(csv_data[\"position\"] == \"dev\",1,\n",
    "                                               np.where(csv_data[\"position\"] == \"manager\",2,3))\n",
    "    return csv_data\n",
    "    \n",
    "\n",
    "def skill_clean(skills_list):\n",
    "    skill_filter = [\"c\", \"c++\", \"java,j2ee\", \"ios,objective-c,swift\", \"sql,database\", \"python\", \"linux\",\n",
    "                    \"software engineering,project management\", \"team building\", \"customer service\",\n",
    "                    \"test management,test plan\", \"test,test automation\", \"qtp\", \"quality assurance\", \"shell,script\"]\n",
    "    skill_clean_ret = []\n",
    "    for skills in skills_list:\n",
    "        ret = 0\n",
    "        skill_list = skills.split(\";\")\n",
    "        # 取前5种技能\n",
    "        skill_list = skill_list[:5]\n",
    "        for skill in skill_list:\n",
    "            skill = skill.replace('+', '\\+')\n",
    "            for index, k in enumerate(skill_filter):\n",
    "                if re.search(skill, k) != None:\n",
    "                    ret += int(math.pow(2, index+1))\n",
    "                    break\n",
    "        skill_clean_ret.append(np.log2(ret+1)) #取2为底的对数，降低数据集。效果会比不取对数好很多\n",
    "    return skill_clean_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logic_predict(train_x,train_y,test_x,test_y):\n",
    "    classifier = LogisticRegression(solver='lbfgs',multi_class='ovr',max_iter=500)\n",
    "    classifier.fit(X=train_x,y=train_y.astype(int))\n",
    "    y_predict = classifier.predict(test_x)\n",
    "\n",
    "    print(\"=== 逻辑回归 =====\")\n",
    "    judge_model(test_y,y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用SVM\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def svm_predict(train_x, train_y, test_x, test_y):\n",
    "    print(\"====== SVC =====\")\n",
    "    clf = SVC(gamma='auto', C=1, decision_function_shape='ovo')\n",
    "    clf.fit(train_x, train_y.astype(int))\n",
    "    print(\"====== 测试集 =====\")\n",
    "    predict_y = clf.predict(test_x)\n",
    "    judge_model(test_y, predict_y)\n",
    "    print(\"====== 训练集 =====\")\n",
    "    predict_y = clf.predict(train_x)\n",
    "    judge_model(train_y, predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用高斯贝叶斯\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def bayes_predict(train_x, train_y, test_x, test_y):\n",
    "    gsnb = GaussianNB()\n",
    "    gsnb.fit(train_x,train_y.astype(int))\n",
    "    predict_y = gsnb.predict(test_x)\n",
    "    print(\"=== 高斯贝叶斯 ====\")\n",
    "    judge_model(test_y,predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据精准率，召回率，FlScore评判模型\n",
    "def judge_model(real, predict):\n",
    "    # 正确率只是想计算一下看下\n",
    "    correct_num = np.sum((real-predict) == 0)\n",
    "    total = np.size(predict, axis=0)\n",
    "    rate = correct_num/total\n",
    "    print((\"正确率为:\"+repr(rate)))\n",
    "\n",
    "    print('=======')\n",
    "    print(\"实际值：\")\n",
    "    print(real)\n",
    "    print(\"预测值：\")\n",
    "    print(predict)\n",
    "    # 从real中获取测试的所有类的取值\n",
    "    class_list = set(real)\n",
    "    print('=======')\n",
    "    print((\"预测的类别有：\"+repr(class_list)))\n",
    "\n",
    "    # 计算每个类的精准率，召回率，FlScore\n",
    "    for c in class_list:\n",
    "        n_pred_c = np.sum(predict == c)\n",
    "        n_real_c = np.sum(real == c)\n",
    "        c_list = np.array([c]*total)\n",
    "        TP = np.sum((predict == c_list) & (\n",
    "            real == c_list) == True)  # 实际为c类，预测也为c类\n",
    "        #FP = np.sum(predict == c ) -TP  #实际不是c类，但被预测为c类\n",
    "#         FN = np.sum( real == c ) - TP #实际是c类，但被预测为其它类\n",
    "        print((\"TP:\"+str(TP)+\",pred_c:\"+str(n_pred_c)+\",real_c:\"+str(n_real_c)))\n",
    "\n",
    "        # 精准率\n",
    "        precision = recall = flscore = 0\n",
    "        if n_pred_c != 0:\n",
    "            precision = TP/n_pred_c\n",
    "        else:\n",
    "            precision = 0\n",
    "        # 召回率\n",
    "        if n_real_c != 0:\n",
    "            recall = float(TP)/n_real_c\n",
    "        else:\n",
    "            recall = 0\n",
    "\n",
    "        if (precision + recall) != 0:\n",
    "            flscore = 2*(precision * recall)/(precision + recall)\n",
    "        else:\n",
    "            flscore = 0\n",
    "\n",
    "        print((\"预测\"+repr(c)+\":\"))\n",
    "        print((\"精准率：\"+str(precision)+\",召回率：\" +\n",
    "               str(recall)+\",flscore:\"+repr(flscore)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 逻辑回归 =====\n",
      "正确率为:0.6666666666666666\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 2 1 2 1 1 2 1 2 2]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:6,pred_c:7,real_c:8\n",
      "预测1:\n",
      "精准率：0.8571428571428571,召回率：0.75,flscore:0.7999999999999999\n",
      "TP:2,pred_c:5,real_c:3\n",
      "预测2:\n",
      "精准率：0.4,召回率：0.6666666666666666,flscore:0.5\n",
      "TP:0,pred_c:0,real_c:1\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      "====== SVC =====\n",
      "====== 测试集 =====\n",
      "正确率为:0.75\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 2 1 1 3 1 2 3]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:7,pred_c:8,real_c:8\n",
      "预测1:\n",
      "精准率：0.875,召回率：0.875,flscore:0.875\n",
      "TP:1,pred_c:2,real_c:3\n",
      "预测2:\n",
      "精准率：0.5,召回率：0.3333333333333333,flscore:0.4\n",
      "TP:1,pred_c:2,real_c:1\n",
      "预测3:\n",
      "精准率：0.5,召回率：1.0,flscore:0.6666666666666666\n",
      "====== 训练集 =====\n",
      "正确率为:0.8666666666666667\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 3 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 3 3 3 1 1 3 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:34,pred_c:39,real_c:37\n",
      "预测1:\n",
      "精准率：0.8717948717948718,召回率：0.918918918918919,flscore:0.8947368421052632\n",
      "TP:14,pred_c:16,real_c:16\n",
      "预测2:\n",
      "精准率：0.875,召回率：0.875,flscore:0.875\n",
      "TP:4,pred_c:5,real_c:7\n",
      "预测3:\n",
      "精准率：0.8,召回率：0.5714285714285714,flscore:0.6666666666666666\n",
      "=== 高斯贝叶斯 ====\n",
      "正确率为:0.5833333333333334\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 3 2 2 2 1 2 2 2 2]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:4,pred_c:4,real_c:8\n",
      "预测1:\n",
      "精准率：1.0,召回率：0.5,flscore:0.6666666666666666\n",
      "TP:3,pred_c:7,real_c:3\n",
      "预测2:\n",
      "精准率：0.42857142857142855,召回率：1.0,flscore:0.6\n",
      "TP:0,pred_c:1,real_c:1\n",
      "预测3:\n",
      "精准率：0.0,召回率：0.0,flscore:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib as plt\n",
    "def main():\n",
    "    data = data_handle()\n",
    "    #将数据分为训练集和测试集\n",
    "    #train,test = train_test_split(data,test_size=0.1,random_state=int(time.time()))\n",
    "    \n",
    "        #因为样本不均匀，随机抽取可能会使某一类不在训练集或全在训练集，所以手动选择\n",
    "    test = data[::6]\n",
    "    train = data.append(test).drop_duplicates(keep = False)\n",
    "\n",
    "    train = np.array(train)\n",
    "    train_x = train[:,5:8]\n",
    "    train_y = train[:,8]\n",
    "    \n",
    "    test = np.array(test)\n",
    "    test_x = test[:,5:8]\n",
    "    test_y = test[:,8]\n",
    "\n",
    "\n",
    "    logic_predict(train_x,train_y,test_x,test_y)\n",
    "    svm_predict(train_x,train_y,test_x,test_y)\n",
    "\n",
    "    bayes_predict(train_x,train_y,test_x,test_y)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己编写逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的大小:61\n",
      "测试集的大小:11\n",
      "=== 逻辑回归 =====\n",
      "正确率为:0.7272727272727273\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 1 1 2 2]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:7,pred_c:9,real_c:7\n",
      "预测1:\n",
      "精准率：0.7777777777777778,召回率：1.0,flscore:0.8750000000000001\n",
      "TP:1,pred_c:2,real_c:3\n",
      "预测2:\n",
      "精准率：0.5,召回率：0.3333333333333333,flscore:0.4\n",
      "TP:0,pred_c:0,real_c:1\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      " ===== 开始 训练 ====== \n",
      "(61, 7)\n",
      "(7,)\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "最终得到的theta为：array([ 2.70624142e-04, -3.26531035e-01, -1.94051969e-01, -1.40964975e+00,\n",
      "        7.47952655e-02,  2.62301630e-01,  4.05328320e-01])\n",
      "1的最终的损失为:0.4985696814176488\n",
      "==== end  ======== \n",
      "(7,)\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "最终得到的theta为：array([-4.17354935e-04,  2.51900671e-01,  8.58543212e-02,  1.22508662e+00,\n",
      "       -1.00263001e-03, -7.00288113e-01, -7.00551872e-01])\n",
      "2的最终的损失为:0.4575282335037266\n",
      "==== end  ======== \n",
      "(7,)\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "最终得到的theta为：array([-9.31564581e-04, -3.49978623e-01,  1.12247952e-01,  7.37721044e-01,\n",
      "       -8.70904772e-01,  1.04799090e-01, -1.52331369e+00])\n",
      "3的最终的损失为:0.31120570305652495\n",
      "==== end  ======== \n",
      "========训练集验证 ：=======\n",
      "类1的损失为：0.5189\n",
      "类2的损失为：0.4883\n",
      "类3的损失为：0.3549\n",
      "正确率为:0.5901639344262295\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 2 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 2 0 2 1 2 0 1 2 0 2 0 0 0 1 0 0 0 0 0 1 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:31,pred_c:37,real_c:38\n",
      "预测1:\n",
      "精准率：0.8378378378378378,召回率：0.8157894736842105,flscore:0.8266666666666665\n",
      "TP:5,pred_c:6,real_c:16\n",
      "预测2:\n",
      "精准率：0.8333333333333334,召回率：0.3125,flscore:0.45454545454545453\n",
      "TP:0,pred_c:0,real_c:7\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      "======= 测试集验证 ：======\n",
      "类1的损失为：0.6014\n",
      "类2的损失为：0.6140\n",
      "类3的损失为：0.4050\n",
      "正确率为:0.5454545454545454\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[0 1 1 1 1 0 1 0 0 2 0]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:5,pred_c:5,real_c:7\n",
      "预测1:\n",
      "精准率：1.0,召回率：0.7142857142857143,flscore:0.8333333333333333\n",
      "TP:1,pred_c:1,real_c:3\n",
      "预测2:\n",
      "精准率：1.0,召回率：0.3333333333333333,flscore:0.5\n",
      "TP:0,pred_c:0,real_c:1\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n"
     ]
    }
   ],
   "source": [
    "# 自己实现逻辑回归\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# 定义梯度下降\n",
    "# X_set 变量集\n",
    "# y_set 结果集\n",
    "# alpha 学习速率\n",
    "\n",
    "\n",
    "def gradient_descent(X_set, y_set, alpha, theta,lamda):\n",
    "    #print(\"==== 梯度 下降 ===== \")\n",
    "    m = np.size(X_set, axis=0)\n",
    "    regular_theta = theta\n",
    "    regular_theta[0] = 0\n",
    "    h_x =1/(1+np.e** -np.dot(X_set , theta))\n",
    "    derivative_sum = np.dot(X_set.transpose(), h_x - y_set)\n",
    "    theta = theta - alpha*(derivative_sum/m + (lamda/m)*regular_theta)\n",
    "    #theta = theta - alpha * derivative_sum/m\n",
    "    return theta\n",
    "\n",
    "# 定义代价函数\n",
    "\n",
    "\n",
    "def cost_function(X_set, y, theta,lamda):\n",
    "   # print(\"==== 代价函数  计算损失==== \")\n",
    "    #cost = -1/m(sum(ylog(h(x) + (1-y)log(1-h(x)))))\n",
    "    m = np.size(X_set, axis=0)  # 样本个数\n",
    "    h_x =1/(1+np.e** -np.dot(X_set , theta))\n",
    "    regular_theta  = theta.copy()\n",
    "    regular_theta[0] = 0\n",
    "    cost = np.sum (y*np.log(h_x) + (1-y)*np.log(1-h_x))/-m+(lamda/(2*m))*np.sum(np.square(regular_theta))\n",
    "    #cost = np.sum (y*np.log(h_x) + (1-y)*np.log(1-h_x))/-m\n",
    "    return cost\n",
    "\n",
    "# 定义训练方法\n",
    "def train(X_train, y_train):\n",
    "    print(\" ===== 开始 训练 ====== \")\n",
    "    \n",
    "\n",
    "    #增加多项式\n",
    "    x3s = X_train[:,2]*X_train[:,2]\n",
    "    x1s = X_train[:,0]*X_train[:,0]\n",
    "    x2s = X_train[:,1]*X_train[:,1]\n",
    "\n",
    "\n",
    "    X_train = np.hstack((X_train,x1s.reshape(-1,1)))\n",
    "    X_train = np.hstack((X_train,x3s.reshape(-1,1)))\n",
    "    X_train = np.hstack((X_train,x2s.reshape(-1,1)))\n",
    "\n",
    "\n",
    "    # 初始化第0列全为1，作为theta_0的变量\n",
    "    X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "    dimention = np.size(X_train, axis=1)  # X的维度\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    #获取所有y的取值,然后将y的结果集拆分成n个结果集，每个结果集判断一种y的取值，是为1，不是为0\n",
    "    y_set = set(y_train)\n",
    "    \n",
    "    theta_dict  = {}\n",
    "    for y in y_set :\n",
    "        y_train_new = np.where (y_train== y , 1 , 0)\n",
    "        theta = np.zeros(dimention) # 初始化theta变量全为0\n",
    "        print(theta.shape)\n",
    "        print(theta)\n",
    "        \n",
    "        #训练单个分类模型\n",
    "        theta = sub_train ( X_train , y_train_new,theta,y)\n",
    "        theta_dict[y] = theta\n",
    "    return theta_dict\n",
    "        \n",
    "def sub_train ( X_train, y_train , theta,y) :\n",
    "        num_iters = 500  # 迭代次数\n",
    "        alpha = 0.05\n",
    "        cost_plot = list()\n",
    "        iteration = range(1, num_iters)\n",
    "        lamda = 0.0     \n",
    "        for i in iteration:\n",
    "            theta = gradient_descent(X_train, y_train, alpha, theta,lamda)\n",
    "            # 计算损失\n",
    "            cost = cost_function(X_train, y_train, theta,lamda)\n",
    "            cost_plot.append(cost)\n",
    "            #print(\"第\"+repr(i)+\"迭代之后的 损失为：\" + repr(cost))\n",
    "\n",
    "    # 画出损失函数图\n",
    "        plt.xlim(0, num_iters)\n",
    "        plt.plot(iteration, cost_plot)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"cost\")\n",
    "    \n",
    "        print ( \"最终得到的theta为：\"+ repr(theta))\n",
    "        print (repr(y)+\"的最终的损失为:\"+repr(cost))\n",
    "        print(\"==== end  ======== \")\n",
    "        return theta\n",
    "    \n",
    "#预测\n",
    "def predicate ( X_set , theta_dict) :\n",
    "    m = np.size(X_set, axis=0)  # 样本个数\n",
    "    x3s = X_set[:,2]*X_set[:,2]\n",
    "    x1s = X_set[:,0]*X_set[:,0]\n",
    "    x2s = X_set[:,1]*X_set[:,1]\n",
    "    X_set = np.hstack((X_set,x1s.reshape(-1,1)))\n",
    "    X_set = np.hstack((X_set,x3s.reshape(-1,1)))\n",
    "    X_set = np.hstack((X_set,x2s.reshape(-1,1)))\n",
    "\n",
    "\n",
    "    X_set = np.insert ( X_set , 0 , 1 , axis = 1)\n",
    "    \n",
    "    last_y_pred = np.zeros(m) #默认初始化预测值为0\n",
    "    #循环遍历每个取值的概率模型，取取值最大的那个，再判断最大的那个的概率是否大于0.5\n",
    "    #这里是假定取值y_key 为整数\n",
    "    for y_key in theta_dict :\n",
    "        theta = theta_dict.get(y_key)\n",
    "        y_pred = 1 / (1 + np.e ** -np.dot(X_set, theta))\n",
    "        cost = cost_function(X_set,y_pred,theta,0.0)\n",
    "        print(\"类%d的损失为：%.4f\" %(y_key ,cost))\n",
    "        last_y_pred = np.where(y_pred > (last_y_pred-last_y_pred.astype(int)) , y_key + y_pred,last_y_pred)\n",
    "    last_y_pred = np.where ( last_y_pred - last_y_pred.astype(int) > 0.5, last_y_pred.astype(int), 0)\n",
    "    \n",
    "    \n",
    "    return last_y_pred\n",
    "    \n",
    "# 定义主函数\n",
    "def my_main():\n",
    "    data = data_handle()\n",
    "     # 将数据分为训练集和测试集\n",
    "#     train_set, test_set  = train_test_split(\n",
    "#         data, test_size=0.1, random_state=int(time.time()))\n",
    "    #因为样本不均匀，随机抽取可能会使某一类不在训练集或全在训练集，所以手动选择\n",
    "    test_set = data[::7]\n",
    "    train_set = data.append(test_set).drop_duplicates(keep = False)\n",
    "    \n",
    "\n",
    "    train_set = np.array(train_set)\n",
    "    train_X = train_set [:, 5:8]\n",
    "    train_y = train_set [:, 8]\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_X = test_set[:, 5:8]\n",
    "    test_y = test_set[:, 8]\n",
    "    \n",
    "    print(\"训练集的大小:\"+repr(np.size(train_X,axis=0)))\n",
    "    print(\"测试集的大小:\"+repr(np.size(test_X,axis=0)))\n",
    "    logic_predict(train_X,train_y,test_X,test_y)\n",
    "    \n",
    "    train_X,mean,std = data_nomalization(train_X.astype(float))\n",
    "    test_X = (test_X - std)/mean\n",
    "\n",
    "    theta = train (train_X.astype(float) , train_y)\n",
    "    \n",
    "    print(\"========训练集验证 ：=======\")\n",
    "    y_pred = predicate(train_X.astype(float) , theta)\n",
    "    judge_model (train_y , y_pred )\n",
    "\n",
    "    \n",
    "    print(\"======= 测试集验证 ：======\")\n",
    "    y_pred = predicate(test_X.astype(float) , theta)\n",
    "    judge_model (test_y , y_pred )\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "my_main ( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己实现贝叶斯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 高斯贝叶斯 ====\n",
      "正确率为:1.0\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:7,pred_c:7,real_c:7\n",
      "预测1:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n",
      "TP:3,pred_c:3,real_c:3\n",
      "预测2:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n",
      "TP:1,pred_c:1,real_c:1\n",
      "预测3:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n",
      "==== bayes  train =======\n",
      "样本总个数:61\n",
      "训练集验证 ：\n",
      "===== 预测  ====== \n",
      "{1: 0.6229508196721312, 2: 0.26229508196721313, 3: 0.11475409836065574}\n",
      "正确率为:0.6721311475409836\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[1 2 1 1 1 2 2 1 1 2 1 1 1 2 1 3 1 1 2 1 2 1 1 2 2 2 2 3 1 1 2 1 1 1 2 1 1\n",
      " 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 2 1 2]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:23,pred_c:25,real_c:38\n",
      "预测1:\n",
      "精准率：0.92,召回率：0.6052631578947368,flscore:0.7301587301587301\n",
      "TP:14,pred_c:29,real_c:16\n",
      "预测2:\n",
      "精准率：0.4827586206896552,召回率：0.875,flscore:0.6222222222222222\n",
      "TP:4,pred_c:7,real_c:7\n",
      "预测3:\n",
      "精准率：0.5714285714285714,召回率：0.5714285714285714,flscore:0.5714285714285714\n",
      "测试集验证 ：\n",
      "===== 预测  ====== \n",
      "{1: 0.6229508196721312, 2: 0.26229508196721313, 3: 0.11475409836065574}\n",
      "正确率为:0.9090909090909091\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 2 1 2 2 2 3]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:6,pred_c:6,real_c:7\n",
      "预测1:\n",
      "精准率：1.0,召回率：0.8571428571428571,flscore:0.923076923076923\n",
      "TP:3,pred_c:4,real_c:3\n",
      "预测2:\n",
      "精准率：0.75,召回率：1.0,flscore:0.8571428571428571\n",
      "TP:1,pred_c:1,real_c:1\n",
      "预测3:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n"
     ]
    }
   ],
   "source": [
    "# 自己实现贝叶斯分类模型\n",
    "# 假设P(Xi|C) 符合高斯分布\n",
    "\n",
    "\n",
    "def bayes_train(X_train, y_train):\n",
    "    print(\"==== bayes  train =======\")\n",
    "    # 由贝叶斯的似然函数可知道，假设k个类C，特征值X有n个，则theta矩阵为 k*n或n*k\n",
    "    # 因为为高斯分布，所以每一个theta有两个参数值 u和sigma^2\n",
    "\n",
    "    class_y = set(y_train)  # 统计分类\n",
    "    class_cnt = len(class_y)  # 分类的个数\n",
    "    feature_cnt = np.size(X_train, axis=1)  # X的维度即特征值的个数\n",
    "    \n",
    "    m = np.size ( X_train , axis=0) #样本个数\n",
    "    print(\"样本总个数:\"+repr(m))\n",
    "    class_theta_map = {} #类与theta参数的对应关系\n",
    "    class_p_map = {}  #类的概率\n",
    "    for y in class_y:\n",
    "        sample_rows = np.where(y_train==y)  # 返回y_train结果集中为y类的行号\n",
    "\n",
    "        # 根据行号取出对应的特征集\n",
    "        X_sample = X_train[sample_rows]\n",
    "        mc = np.size(X_sample, axis=0)\n",
    "\n",
    "        # 求均值\n",
    "        oneclass_mean_matrix = np.sum(X_sample, axis=0)/mc  # 得到一个类别在各个特征值上的均值\n",
    "\n",
    "\n",
    "        # 求sigma^2\n",
    "        oneclass_sigma_square_matrix = np.sum(\n",
    "            np.square(X_sample - oneclass_mean_matrix), axis=0)/mc\n",
    "        #合并两个参数 得到一个类似[[std_1,sigma_1],[std_2,sigma_2] ...]\n",
    "        oneclass_theta = np.array(np.vstack((oneclass_mean_matrix,oneclass_sigma_square_matrix)))\n",
    "        class_theta_map [y] = oneclass_theta\n",
    "        \n",
    "        P_c = mc / m #C类出现的概率\n",
    "        class_p_map[y] = P_c\n",
    "        \n",
    "    return class_theta_map,class_p_map\n",
    "\n",
    "#预测\n",
    "def predicate(X_set , class_theta_map , class_p_map) :\n",
    "    print(\"===== 预测  ====== \")\n",
    "    m = np.size(X_set, axis=0)\n",
    "    result = pd.DataFrame(np.zeros((m,2)),columns=[\"p\",\"class\"]) #初始\n",
    "    #循环计算每个类的概率,去掉1/Z\n",
    "    \n",
    "    print(class_p_map)\n",
    "    for c in class_theta_map :\n",
    "        P_c = class_p_map[c]\n",
    "        theta = class_theta_map[c]\n",
    "        mean = theta[0]\n",
    "        sigma_square = theta[1]\n",
    "        sigma = np.sqrt(sigma_square)\n",
    "        const_num = np.sqrt(2*np.pi)\n",
    "        #print(const_num)\n",
    "        P_x_c = np.prod ((1/(const_num*sigma))*(np.e ** -np.square(X_set - mean)/(2*sigma_square)),axis=1)\n",
    "        #print(P_x_c)\n",
    "        P_c_x = P_c*P_x_c.astype(float)\n",
    "     \n",
    "        result[\"class\"] = np.where(result[\"p\"] < P_c_x, c, result[\"class\"]).astype(int)\n",
    "        result[\"p\"] = np.where(result[\"p\"] < P_c_x, P_c_x, result[\"p\"]) #取概率大的那个\n",
    "    return np.array(result[\"class\"].astype(int) )    \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# 定义主函数\n",
    "def bayes_main():\n",
    "    data = data_handle()\n",
    "    # 将数据分为训练集和测试集\n",
    "      #因为样本不均匀，随机抽取可能会使某一类不在训练集或全在训练集，所以手动选择\n",
    "    test_set = data[::7]\n",
    "    train_set = data.append(test_set).drop_duplicates(keep = False)\n",
    "\n",
    "    train_set = np.array(train_set)\n",
    "    train_X = train_set [:, 5:8]\n",
    "    train_y = train_set [:, 8]\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_X = test_set[:, 5:8]\n",
    "    test_y = test_set[:, 8]\n",
    "    \n",
    "    bayes_predict(train_X,train_y,test_X,test_y)\n",
    "    \n",
    "    theta,class_p = bayes_train (train_X.astype(float) , train_y)\n",
    "    \n",
    "    print(\"训练集验证 ：\")\n",
    "    y_pred = predicate(train_X , theta,class_p)\n",
    "    judge_model (train_y , y_pred )\n",
    "    \n",
    "    print(\"测试集验证 ：\")\n",
    "    y_pred = predicate(test_X , theta,class_p)\n",
    "    judge_model (test_y , y_pred )\n",
    "    \n",
    "    \n",
    "bayes_main ( )        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己实现SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     41
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 单个子SVM\n",
    "class SubSVM:\n",
    "    # 成员变量\n",
    "    w = 0  # 最大超平面的w向量值\n",
    "    b = 0  # 最大超平面的b值\n",
    "    category = 0\n",
    "    # 构造函数，初始化，传入类别\n",
    "\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "        matplotlib.use('TkAgg')\n",
    "\n",
    "    # 训练\n",
    "    def train(self, X_train, y_train):\n",
    "        m = np.size(X_train, axis=0)  # 样本个数\n",
    "        # 使用SMO算法最优化对偶问题，求得最优的alpha\n",
    "        C = 1.0\n",
    "        max_iter = 50\n",
    "        optimize_alpha,b = self.smo_simple(X_train, y_train, C, max_iter)\n",
    "        # 求w\n",
    "        self.w = np.dot(X_train.transpose(), optimize_alpha*y_train)\n",
    "        # 求b，由KKT条件得：(1-y(wx+b)) = 0。求所有支持向量的均值\n",
    "        self.b = np.sum(y_train - np.dot(X_train, self.w))/m\n",
    "        print(\"==== w === \")\n",
    "        print(self.w)\n",
    "        print(\"====== b :\"+repr(self.b))\n",
    "\n",
    "    # SMO算法\n",
    "    def smo_simple(self, X_train, y_train, C, max_iter):\n",
    "        print(\"====== SMO ===== \")\n",
    "        m, n = X_train.shape\n",
    "        alpha = np.zeros(m).astype(float)  # 初始化alpha\n",
    "        b = 0\n",
    "        iters = 0\n",
    "        cost_plot = list()\n",
    "        iteration = range(1,max_iter+1)\n",
    "        while iters < max_iter:\n",
    "            iters += 1\n",
    "            for i in range(0, m):\n",
    "                # 求f(xi)的值\n",
    "                fxi = np.sum(alpha * y_train *\n",
    "                             np.dot(X_train, X_train[i].transpose()))+b\n",
    "                # 求实际值与预测值之间的差\n",
    "                Ei = fxi - y_train[i]\n",
    "\n",
    "                # 随机选取第二个参数j， j!=i\n",
    "                j = self.select_j_rand(i, m)\n",
    "                fxj = np.sum(alpha * y_train *\n",
    "                             np.dot(X_train, X_train[j].transpose()))+b\n",
    "                Ej = fxj - y_train[j]\n",
    "\n",
    "                # Kii+Kjj-2Kij\n",
    "                eta = np.dot(X_train[i], X_train[i].transpose())+np.dot(X_train[j],\n",
    "                                                                        X_train[j].transpose()) - 2*np.dot(X_train[i], X_train[j].transpose())\n",
    "                \n",
    "                if(eta == 0) :\n",
    "                    continue\n",
    "                alpha_j_new = (\n",
    "                    y_train[j]*(Ei - Ej))/eta + alpha[j]\n",
    "                \n",
    "                alpha_i_old = alpha[i].copy()\n",
    "                alpha_j_old = alpha[j].copy()\n",
    "                \n",
    "                # 根据约束条件裁剪alpha_j_new\n",
    "                alpha_j_new,L,H = self.clipped_alpha(\n",
    "                    alpha_j_new, y_train[i], y_train[j], alpha[i], alpha[j], C)\n",
    "                if ( L == H ) :\n",
    "                  #  print(\"第\"+repr(iters)+\"次循环:L=H=\"+repr(L)+\",i=\"+repr(i))\n",
    "                    continue\n",
    "        \n",
    "          \n",
    "      \n",
    "                alpha[j] = alpha_j_new\n",
    "        \n",
    "                if(abs(alpha_j_new - alpha_j_old) < 0.0001) :\n",
    "                    #print(\"第\"+repr(iters)+\"次循环,改变很小，无需优化\"+repr(j))\n",
    "                    continue\n",
    "                # 根据alpha_j 求alpha_i\n",
    "                alpha_i_new = alpha[i] + (alpha_j_old - alpha_j_new)*y_train[i]*y_train[j]\n",
    "\n",
    "              # 更新alpha\n",
    "                alpha[i] = alpha_i_new\n",
    "                \n",
    "\n",
    "                # 更新b\n",
    "                b1 = (alpha_i_old-alpha[i])*y_train[i]*np.dot(X_train[i], X_train[i].transpose())+(\n",
    "                    alpha_j_old-alpha[j])*y_train[j] * np.dot(X_train[j], X_train[i].transpose())+b-Ei\n",
    "\n",
    "                b2 = (alpha_i_old-alpha[i])*y_train[i]*np.dot(X_train[i], X_train[j].transpose())+(\n",
    "                    alpha_j_old-alpha[j])*y_train[j] * np.dot(X_train[j], X_train[j].transpose())+b-Ej\n",
    "\n",
    "                if 0< alpha[i] and C > alpha[i]:\n",
    "                    b = b1\n",
    "                elif 0< alpha[j] and C > alpha[j]:\n",
    "                    b = b2\n",
    "                else:\n",
    "                    b = (b1+b2)/2\n",
    "            #计算损失\n",
    "            cost = self.cost_function(X_train,y_train,b,alpha)\n",
    "            cost_plot.append(cost)\n",
    "        print(\"==== 最终损失为：\"+repr(cost))\n",
    "        # 画出损失函数图\n",
    "        plt.xlim(0, max_iter)\n",
    "        plt.plot(iteration,cost_plot)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"cost\")\n",
    "\n",
    "    \n",
    "        return alpha,b\n",
    "\n",
    "    # 随机选择第二个参数的index j\n",
    "    def select_j_rand(self, i, m):\n",
    "        j = i\n",
    "        while (j == i):\n",
    "            j = random.randint(0, m-1)\n",
    "        return j\n",
    "\n",
    "    # 裁剪alpha\n",
    "    def clipped_alpha(self, alpha_new_x, yi, yj, alpha_i, alpha_j, C):\n",
    "        L = H = 0\n",
    "        if yi != yj:           \n",
    "            k = alpha_j - alpha_i\n",
    "            # 确定下界\n",
    "            L = max(k, 0)\n",
    "            # 确定上界\n",
    "            H = min(C, C+k)\n",
    "        else:\n",
    "            k = alpha_i+alpha_j\n",
    "            L = max(0, k - C)\n",
    "            H = min(k, C)\n",
    "        # alpha_new 必须介于L和H之间\n",
    "        alpha_new_x = max(L, alpha_new_x)\n",
    "        alpha_new_x = min(H, alpha_new_x)\n",
    "                \n",
    "        return alpha_new_x,L,H\n",
    "\n",
    "    #计算损失\n",
    "    def cost_function(self , X_train, y_train, b,alpha):\n",
    "        m ,n = X_train.shape\n",
    "        cost =  np.sum(np.square(np.dot( np.dot(X_train, X_train.transpose()), alpha*y_train) + b - y_train))/m\n",
    "        return cost\n",
    "        \n",
    "    # 决策\n",
    "    def sign(self, X):\n",
    "        f_x =  np.dot(X, self.w) + self.b\n",
    "        # 决策函数决策，返回1，0， -1 预测值。1 表示正类。即属于self.category\n",
    "        sign_fx = np.where(f_x < 0, -1, np.where(f_x == 0, 0, 1))\n",
    "        return sign_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== SVC =====\n",
      "====== 测试集 =====\n",
      "正确率为:0.9090909090909091\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 2 2 2 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:7,pred_c:8,real_c:7\n",
      "预测1:\n",
      "精准率：0.875,召回率：1.0,flscore:0.9333333333333333\n",
      "TP:3,pred_c:3,real_c:3\n",
      "预测2:\n",
      "精准率：1.0,召回率：1.0,flscore:1.0\n",
      "TP:0,pred_c:0,real_c:1\n",
      "预测3:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      "====== 训练集 =====\n",
      "正确率为:0.8524590163934426\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 1 1 1]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:34,pred_c:38,real_c:38\n",
      "预测1:\n",
      "精准率：0.8947368421052632,召回率：0.8947368421052632,flscore:0.8947368421052632\n",
      "TP:14,pred_c:18,real_c:16\n",
      "预测2:\n",
      "精准率：0.7777777777777778,召回率：0.875,flscore:0.823529411764706\n",
      "TP:4,pred_c:5,real_c:7\n",
      "预测3:\n",
      "精准率：0.8,召回率：0.5714285714285714,flscore:0.6666666666666666\n",
      "====== 自己写的 SVM ======\n",
      "====== SMO ===== \n",
      "==== 最终损失为：1.147621798650779\n",
      "==== w === \n",
      "[-0.7564659  -0.07494149 -0.28958939]\n",
      "====== b :3.22169744816199\n",
      "====== SMO ===== \n",
      "==== 最终损失为：1.0452640683479308\n",
      "==== w === \n",
      "[6.01894992e-04 1.79009081e-05 2.73088284e-04]\n",
      "====== b :-0.4779973812970438\n",
      "====== SMO ===== \n",
      "==== 最终损失为：0.45896094390398867\n",
      "==== w === \n",
      "[-4.54898033e-04 -1.57586688e-05  2.78888024e-13]\n",
      "====== b :-0.7697007735721397\n",
      "=======训练集验证 ：======\n",
      "===== 预测 ====== \n",
      "==== 预测 :1\n",
      "==== 预测 :2\n",
      "==== 预测 :3\n",
      "正确率为:0.5573770491803278\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 3 3 1 1 1 1 1 1 3 1 3 1 1 1 1 3 1 1 3 3 3 1 3 1 1 3 1 1 1 1 1 1\n",
      " 1 1 3 3 3 3 1 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 1 3]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:28,pred_c:32,real_c:38\n",
      "预测1:\n",
      "精准率：0.875,召回率：0.7368421052631579,flscore:0.7999999999999999\n",
      "TP:0,pred_c:0,real_c:16\n",
      "预测2:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      "TP:6,pred_c:29,real_c:7\n",
      "预测3:\n",
      "精准率：0.20689655172413793,召回率：0.8571428571428571,flscore:0.33333333333333326\n",
      "======= 测试集验证 ：=======\n",
      "===== 预测 ====== \n",
      "==== 预测 :1\n",
      "==== 预测 :2\n",
      "==== 预测 :3\n",
      "正确率为:0.6363636363636364\n",
      "=======\n",
      "实际值：\n",
      "[1 1 1 1 1 1 1 2 2 2 3]\n",
      "预测值：\n",
      "[1 1 1 1 1 3 1 3 3 3 3]\n",
      "=======\n",
      "预测的类别有：{1, 2, 3}\n",
      "TP:6,pred_c:6,real_c:7\n",
      "预测1:\n",
      "精准率：1.0,召回率：0.8571428571428571,flscore:0.923076923076923\n",
      "TP:0,pred_c:0,real_c:3\n",
      "预测2:\n",
      "精准率：0,召回率：0.0,flscore:0\n",
      "TP:1,pred_c:5,real_c:1\n",
      "预测3:\n",
      "精准率：0.2,召回率：1.0,flscore:0.33333333333333337\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "# 线性可分SVM\n",
    "class LinearSVM:\n",
    "    model_list = list()  # 因为SVM是一个二分类，所以通过构造N-1个SubSVM来进行多分类\n",
    "\n",
    "    # 训练\n",
    "    def fit(self, X_train, y_train):\n",
    "        m = np.size(X_train, axis=0)  # 样本个数\n",
    "        # 获取所有y的取值,然后将y的结果集拆分成n个结果集，每个结果集判断一种y的取值，是为1，不是为-1\n",
    "        y_set = set(y_train)\n",
    "        for y in y_set:\n",
    "            y_sub_train = np.where(y_train == y, 1, -1)\n",
    "            subSVM = SubSVM(y)\n",
    "            subSVM.train(X_train, y_sub_train)\n",
    "            self.model_list.append(subSVM)\n",
    "\n",
    "     # 预测\n",
    "\n",
    "    def predict(self, X):\n",
    "        m = np.size(X, axis=0)  # 样本个数\n",
    "        y_pred = np.zeros(m)\n",
    "        print(\"===== 预测 ====== \")\n",
    "        # 让样本经过每一个模型进行预测\n",
    "        need_pred_X = X\n",
    "        result = pd.DataFrame(np.zeros((m,2)),columns=[\"p\",\"class\"]) #初始\\\n",
    "        unjudge_samples = np.array(range(0,m))\n",
    "        for index, model in enumerate(self.model_list):\n",
    "            \n",
    "            print(\"==== 预测 :\"+repr(model.category))\n",
    "            sign_x = model.sign(need_pred_X)\n",
    "#             result[\"class\"] = np.where(result[\"p\"] <= sign_x, model.category, result[\"class\"])\n",
    "#             result[\"p\"] = np.where(result[\"p\"] <= sign_x, sign_x, result[\"p\"])\n",
    "            \n",
    "            if  index == (len(self.model_list) - 1):\n",
    "                # 最后一个模型，无需再分类\n",
    "                y_pred[unjudge_samples] = model.category\n",
    "            else:\n",
    "                negtive_columns=np.where(sign_x != 1)\n",
    "                positive_columns=np.where(sign_x == 1)\n",
    "\n",
    "                need_pred_X = X[negtive_columns]\n",
    "                y_pred[unjudge_samples[positive_columns]] = model.category\n",
    "                unjudge_samples = unjudge_samples[negtive_columns]\n",
    "        return y_pred.astype(int)\n",
    "\n",
    "def my_svm_main() :\n",
    "    \n",
    "    data = data_handle()\n",
    "    # 将数据分为训练集和测试集\n",
    "      #因为样本不均匀，随机抽取可能会使某一类不在训练集或全在训练集，所以手动选择\n",
    "    test_set = data[::7]\n",
    "    train_set = data.append(test_set).drop_duplicates(keep = False)\n",
    "\n",
    "    train_set = np.array(train_set)\n",
    "    train_X = train_set [:, 5:8]\n",
    "    train_y = train_set [:, 8]\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_X = test_set[:, 5:8]\n",
    "    test_y = test_set[:, 8]\n",
    "    \n",
    "    svm_predict(train_X,train_y,test_X,test_y)\n",
    "#     train_X,mean,std = data_nomalization(train_X.astype(float))\n",
    "#     test_X = (test_X-mean)/std\n",
    "      \n",
    "    print(\"====== 自己写的 SVM ======\")\n",
    "    mysvm = LinearSVM()\n",
    "    mysvm.fit(train_X.astype(float), train_y)\n",
    "    \n",
    "    print(\"=======训练集验证 ：======\")\n",
    "    \n",
    "    y_pred = mysvm.predict(train_X)\n",
    "    judge_model (train_y , y_pred )\n",
    "    \n",
    "    print(\"======= 测试集验证 ：=======\")\n",
    "    y_pred = mysvm.predict(test_X)\n",
    "    judge_model (test_y , y_pred )\n",
    "    \n",
    "my_svm_main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
